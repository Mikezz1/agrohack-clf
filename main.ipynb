{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta? .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "161c945532039f7d57aa33e4253d854ac273b60397325c019a7bd57c2c7e143b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUjItgNUXrrw"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pCcCM8rX_Ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634f619c-1d7e-4d39-81d9-ec1f893e0ed6"
      },
      "source": [
        "!pip install transformers nlpaug"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.7/dist-packages (1.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug) (1.3.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->nlpaug) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqdLEVF4Xrr0"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from transformers import Trainer\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoTokenizer\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czwAu3R2vuEd"
      },
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijFBgzVfXrr2"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_7ZaDm3Xrr3"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FbaZwHrXrr5"
      },
      "source": [
        "# Base model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF3gGicQXrr6"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(pd.DataFrame(train['text']), train[train.columns[2:]], test_size=0.2)\n",
        "\n",
        "y_train = y_train.reset_index().drop('index',axis = 1)\n",
        "y_val = y_val.reset_index().drop('index',axis = 1)\n",
        "\n",
        "X_train = X_train.reset_index().drop('index',axis = 1)\n",
        "X_val = X_val.reset_index().drop('index',axis = 1)\n",
        "\n",
        "X_test = pd.DataFrame(test['text'])\n",
        "y_test = pd.DataFrame(np.zeros((X_test.shape[0],11)))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3YUPyr2U10"
      },
      "source": [
        "import nlpaug.augmenter.word as naw\n",
        "aug = naw.RandomWordAug(aug_p = 0.3)\n",
        "\n",
        "augmented_random = np.array(aug.augment(list(X_train['text'].values)))\n",
        "\n",
        "b = pd.DataFrame(columns = X_train.columns)\n",
        "b['text'] = augmented_random\n",
        "\n",
        "X_train = X_train.append(b, ignore_index=True)\n",
        "y_train = y_train.append(y_train, ignore_index=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "TXYu8Enn4z7e",
        "outputId": "c704619b-f106-40f2-9f24-47ac7bef35b8"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Как производить профилактику расстройства пище...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Появились пролысины на шее у коровы.\\nПодскажи...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Подскажите советом!!! Теленку 3 недели взяли д...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>У лейкозной коровы язвочки на сосках, что это?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Тёлка месячная отказывается есть и поносит, чт...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>Здравствуйте форумчане, хочу корову, но столкн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>всем привет, утром обнаружил слабость у телки,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>У у телки на шее шишка, и язвы начало еще боль...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>Телята дорастают до определённого возраста и п...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>Что делать, если у коровы запуске одна вымени ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>470 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "0    Как производить профилактику расстройства пище...\n",
              "1    Появились пролысины на шее у коровы.\\nПодскажи...\n",
              "2    Подскажите советом!!! Теленку 3 недели взяли д...\n",
              "3    У лейкозной коровы язвочки на сосках, что это?...\n",
              "4    Тёлка месячная отказывается есть и поносит, чт...\n",
              "..                                                 ...\n",
              "465  Здравствуйте форумчане, хочу корову, но столкн...\n",
              "466  всем привет, утром обнаружил слабость у телки,...\n",
              "467  У у телки на шее шишка, и язвы начало еще боль...\n",
              "468  Телята дорастают до определённого возраста и п...\n",
              "469  Что делать, если у коровы запуске одна вымени ...\n",
              "\n",
              "[470 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1sDgix9Xrr7"
      },
      "source": [
        "**Обучаем модель**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXRA_XjYXrr7"
      },
      "source": [
        "MODEL_NAME = \"sberbank-ai/ruRoberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fml6QLAcXrr8",
        "outputId": "f19784ba-d869-4ccb-d10a-e2f1111bbd97"
      },
      "source": [
        "tokenized = list()\n",
        "[tokenized.append(tokenizer.tokenize(t)) for t in X_train.text]\n",
        "tokenized = np.array(tokenized)\n",
        "length = [len(i) for i in tokenized]\n",
        "l = np.percentile(length,95)\n",
        "MAX_LEN = int(l)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPE1uUnRXrr8"
      },
      "source": [
        "\n",
        "class CowDataset(Dataset):\n",
        "    def __init__(self, ids, text, targets, tokenizer, max_len):\n",
        "        self.ids = ids\n",
        "        self.text = text\n",
        "        self.targets = targets.values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.text[item])\n",
        "        target = self.targets[item]\n",
        "        ids = self.ids[item]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "        )\n",
        "        return {\n",
        "            'id': ids,\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def create_data_loader(ids, text, targets, tokenizer, batch_size, max_len):\n",
        "    dataset = CowDataset(\n",
        "        ids = ids,\n",
        "        text= text,\n",
        "        targets=targets,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "\n",
        "class CowDoctorlassifier(nn.Module):\n",
        "    def __init__(self, n_classes, not_use_pool=False):\n",
        "        super(CowDoctorlassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 512)\n",
        "        self.fc2 = nn.Linear(512, n_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.not_use_pool = not_use_pool\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "\n",
        "        last_hidden_state = outputs[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        last_hidden_state[input_mask_expanded == 0] = -1e9 \n",
        "        max_emb = torch.max(input_mask_expanded * last_hidden_state, 1)[0]\n",
        "        emb = max_emb\n",
        "\n",
        "        if self.not_use_pool:\n",
        "          last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "          emb = last_hidden_state_cls\n",
        "\n",
        "        out =  self.fc2(\n",
        "                self.relu(\n",
        "                    self.drop(\n",
        "                        self.fc1(emb)\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        return {\n",
        "            'logits' : out\n",
        "        }"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW5X35UrZ3Hi"
      },
      "source": [
        "train_ds = CowDataset(\n",
        "    ids=X_train.index.to_numpy(),\n",
        "    text=X_train.text,\n",
        "    targets=y_train,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "    )\n",
        "eval_ds = CowDataset(\n",
        "    ids=X_val.index.to_numpy(),\n",
        "    text=X_val.text,\n",
        "    targets=y_val,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "    )\n",
        "test_ds = CowDataset(\n",
        "    ids=X_test.index.to_numpy(),\n",
        "    text=X_test.text,\n",
        "    targets=y_test,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=MAX_LEN\n",
        "    )\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N297_QSXrr9"
      },
      "source": [
        "class MultilabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(inputs.get('input_ids'), inputs.get('attention_mask'))\n",
        "        logits = outputs.get('logits')\n",
        "        loss_fct = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, 11),\n",
        "                        labels.float().view(-1, 11))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def log_loss_score(gt, pr):\n",
        "    \n",
        "    log_loss_ = 0\n",
        "    \n",
        "    gt = np.array(gt)\n",
        "    \n",
        "    for i in range(10):\n",
        "        log_loss_ += log_loss(gt[:, i], pr[:, i])\n",
        "        \n",
        "    return log_loss_ / 10"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1-Cy45qqdN2"
      },
      "source": [
        "import gc \n",
        "gc.collect() \n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCxMON28Xrr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42fa683-69f4-48a0-8814-e2d1b49697e5"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model = CowDoctorlassifier(n_classes=11, not_use_pool = True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    learning_rate = 2e-5,\n",
        "    eval_steps=25,\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=6,\n",
        "    seed=SEED,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = MultilabelTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset= eval_ds,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQSfWyeZEsde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56d0b52c-e206-4096-bb2d-4abcd8b23406"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 470\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 354\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='354' max='354' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [354/354 04:21, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.501800</td>\n",
              "      <td>0.428977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.437200</td>\n",
              "      <td>0.420348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.394200</td>\n",
              "      <td>0.370766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.354600</td>\n",
              "      <td>0.318445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.285400</td>\n",
              "      <td>0.284371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.277428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.206300</td>\n",
              "      <td>0.255368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.176500</td>\n",
              "      <td>0.253878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.166700</td>\n",
              "      <td>0.248366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.145700</td>\n",
              "      <td>0.248380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.140900</td>\n",
              "      <td>0.250211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.130900</td>\n",
              "      <td>0.250648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.125800</td>\n",
              "      <td>0.250233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.127100</td>\n",
              "      <td>0.250365</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=354, training_loss=0.2432732750466988, metrics={'train_runtime': 261.9125, 'train_samples_per_second': 10.767, 'train_steps_per_second': 1.352, 'total_flos': 0.0, 'train_loss': 0.2432732750466988, 'epoch': 6.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "6YGLr8LecU4u",
        "outputId": "b26df4a2-6f42-4061-b108-f161b47c5788"
      },
      "source": [
        "out,_,metrics = trainer.predict(eval_ds)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 59\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA7KvN6hgBze",
        "outputId": "c67e5a2b-14c8-4004-bd68-680a242c36bf"
      },
      "source": [
        "logits = torch.sigmoid(torch.tensor(out))\n",
        "logits = logits.numpy()\n",
        "(1-log_loss_score(y_val, logits))*0.8"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6104464908676633"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J8Mx3bQXrsC"
      },
      "source": [
        "# Submisson\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "BGJlG9vehxma",
        "outputId": "dae26bcd-e890-4499-9007-ce7c6e4f5983"
      },
      "source": [
        "submit, _, _ = trainer.predict(test_ds)\n",
        "submit = torch.sigmoid(torch.tensor(submit)).numpy()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 99\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOGh8W3oXrsC"
      },
      "source": [
        "submission_columns = ['text_id'] + list(train.columns[2:-1])\n",
        "submission = pd.concat([test['text_id'], pd.DataFrame(submit[:,:10])], axis=1)\n",
        "submission.columns = submission_columns"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNgUo0rhXrsD",
        "outputId": "7366a6fd-ee81-499f-ee51-72c10dc01f43"
      },
      "source": [
        "submission_json = {str(k): {\"span\": list(), \"label\": list(v.values())} \\\n",
        "                   for k,v in submission.set_index('text_id').to_dict('index').items()}\n",
        "\n",
        "submission_json['294']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': [0.9175109267234802,\n",
              "  0.043323397636413574,\n",
              "  0.8142229914665222,\n",
              "  0.8065071105957031,\n",
              "  0.05114462599158287,\n",
              "  0.0673268660902977,\n",
              "  0.046984218060970306,\n",
              "  0.05588258430361748,\n",
              "  0.04735879600048065,\n",
              "  0.04344747215509415],\n",
              " 'span': []}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "188gcq7FXrsE"
      },
      "source": [
        "with open('sample_submission.json', 'w') as final_submit:\n",
        "    json.dump(submission_json, final_submit, indent=4)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmD3azenwOE5"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}